{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c561b273-8ceb-4036-8e13-eca2ddb8614f",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b8c8b-cc30-457b-9487-597c6ff052a8",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7236a4-5e09-4635-ad0d-b84492fe91af",
   "metadata": {},
   "source": [
    "Web scraping is the automated process of extracting information from websites. It involves using specialized software or tools to retrieve data from web pages by sending HTTP requests, parsing the HTML content, and then extracting the desired information. Web scraping allows you to gather data from various websites without having to manually visit each site and copy-paste the information.\n",
    "\n",
    "Web scraping is used for a variety of purposes, including:\n",
    "\n",
    "1. Data Collection and Analysis: Web scraping is commonly used to collect large amounts of data from the internet for analysis. This could involve scraping news articles, social media posts, product prices, reviews, and more. Businesses and researchers use this data to gain insights into market trends, customer sentiment, and other relevant information.\n",
    "\n",
    "2. Competitive Intelligence: Companies often use web scraping to monitor their competitors' websites. By tracking changes in pricing, product offerings, and other information, businesses can adjust their strategies to stay competitive in the market.\n",
    "\n",
    "3. Research and Academia: Researchers and academics use web scraping to gather data for their studies and analyses. This could involve scraping data related to scientific publications, social trends, or any other relevant information that contributes to their research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f22dcc-f5ff-4bd5-8c78-db7de3eb1a9b",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6eeb1-aeca-4234-88c9-d461024acb8e",
   "metadata": {},
   "source": [
    "Q2 What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212806-2658-4da5-add7-1519c6eabd3e",
   "metadata": {},
   "source": [
    "In the web scraping the following methods are used:\n",
    "    1. BeutifulSoup\n",
    "    2. urlOpen\n",
    "    3. request.get\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d17c0-2fba-460c-97e5-1d4ec39c4158",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134591-f070-4209-a482-2982bf62018a",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b52cd-f214-4c5b-853c-a17b84e4ce22",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that is widely used for web scraping purposes. It provides tools for parsing HTML and XML documents, navigating through their structure, and extracting specific information from them. Beautiful Soup simplifies the process of extracting data from web pages by abstracting away the complexities of HTML parsing and manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af196e0a-db8d-4e0d-8a32-f7bcafeb149e",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f9e67-2d58-4de1-85ae-fe946752cc2b",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29232699-3eb5-4114-9ed7-e3eede78599d",
   "metadata": {},
   "source": [
    "Flask is a micro web framework for Python that is often used to build web applications and APIs. When it comes to a web scraping project, Flask can be used for various reasons:\n",
    "\n",
    "1. Displaying Scraped Data: Flask can help you create a simple web interface to display the data you've scraped from websites. You can present the data in a user-friendly format, making it easier for users to interact with and understand.\n",
    "\n",
    "2. Data Visualization: Flask can be combined with data visualization libraries like Matplotlib or Plotly to create dynamic charts, graphs, and visualizations based on the scraped data. This can provide insights and make the data more engaging.\n",
    "\n",
    "3. User Interaction: If you want users to interact with the scraping process, Flask can be used to build forms or input fields that allow users to specify parameters or URLs for scraping. This way, users can customize what data they want to scrape.\n",
    "\n",
    "4. API Endpoints: Flask can be used to create RESTful APIs that serve the scraped data to other applications or clients. This is especially useful if you want to share the scraped data with other systems.\n",
    "\n",
    "5. Automation and Scheduling: You can use Flask in combination with other tools like Celery to create automated scraping tasks that run at specific intervals or based on certain triggers.\n",
    "\n",
    "6. Authentication and Authorization: If you want to restrict access to the scraped data, you can implement authentication and authorization mechanisms using Flask. This ensures that only authorized users can view or interact with the data.\n",
    "\n",
    "7. Error Handling and Reporting: Flask can be used to implement error handling and reporting functionalities. If a scraping task encounters an error, you can display meaningful error messages to users or log them for later analysis.\n",
    "\n",
    "8. Integration with Databases: Flask can be used to store the scraped data in databases like SQLite, MySQL, or PostgreSQL. This allows you to store and manage the data more effectively, and users can retrieve the data from the database using the web interface or API.\n",
    "\n",
    "9. Customization: Flask is highly customizable, allowing you to design the user interface and the behavior of your scraping application exactly as you want it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c49f40-4b24-4136-805c-030aafbf9048",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f10e1-da9c-460a-98e2-f2e34ec14ccf",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276720e-0a4d-4a4f-a096-16d2e9c72872",
   "metadata": {},
   "source": [
    "In this project Elastic Bluestack and Code pipeline are the two AWS servises are used.                            \n",
    " 1. Elastic Bluestack :- it is used to create a enviroment where we can deploy the web application.\n",
    " 2. Code Pipeline :- The code pipeline are used to connect the code sorce like github repository to the bluestack envoroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda6f38-870a-4a5a-b246-7d26804dc9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
